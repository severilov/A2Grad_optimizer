{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fiSHwkvZumRd"
   },
   "source": [
    "# Experiments on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizers import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p2Mo1UdyumT9",
    "outputId": "f14053f0-1237-42a4-f9ee-f5c16460b496"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./MNIST\"\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6Lk5gvGu1Yc"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rk3Q_Jku2by"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = torchvision.datasets.MNIST(root='../data', \n",
    "                                      train=True, download=True, transform=transform)\n",
    "valid_dataset = torchvision.datasets.MNIST(root='../data', \n",
    "                                           train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='../data', \n",
    "                                     train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bYLS8LcHWv7n"
   },
   "outputs": [],
   "source": [
    "valid_size=0.15\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, \n",
    "               batch_size=batch_size, sampler=train_sampler,\n",
    "               num_workers=2)\n",
    "\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, \n",
    "               batch_size=batch_size, sampler=valid_sampler,\n",
    "               num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "7j9cjtv5u57l",
    "outputId": "2d6a7e4b-aa09-49fd-9dbd-d0e3f65ff814"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  tensor([7, 1, 3, 4])\n",
      "Batch shape:  torch.Size([4, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_batch(batch):\n",
    "    im = torchvision.utils.make_grid(batch)\n",
    "    plt.imshow(np.transpose(im.numpy(), (1, 2, 0)))\n",
    "    \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print('Labels: ', labels)\n",
    "print('Batch shape: ', images.size())\n",
    "show_batch(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THk9uDZku-gM"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnS_0OICbHF4"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6micuYGbGOL"
   },
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LR, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = F.log_softmax(self.linear1(x.view(batch_size, -1)), -1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A4rjRazJa-S6"
   },
   "source": [
    "###   Two-layer neural network (fully connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uh3HnmUmu96Q"
   },
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FC, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 256)\n",
    "        self.linear2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_relu = F.relu(self.linear1(x.view(batch_size, -1)))\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SSTdIH_vKV5"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nMttjLlZrbcd"
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def model_step(model, optimizer, criterion, inputs, labels):\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    if model.training:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "    if optimizer.__class__.__name__ != 'SUG':\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            upd_outputs = model(inputs)\n",
    "            upd_loss = criterion(upd_outputs, labels)\n",
    "            upd_loss.backward()\n",
    "            return upd_loss\n",
    "\n",
    "        optimizer.step(loss, closure)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMgATVQnvIiy"
   },
   "outputs": [],
   "source": [
    "def train(model, trainloader, criterion, optimizer, n_epochs=2, validloader=None, eps=1e-5, print_every=1):\n",
    "    tr_loss, val_loss, lips, times, grad, acc = ([] for i in range(6))\n",
    "    start_time = time.time()\n",
    "    model.to(device=device)\n",
    "    for ep in range(n_epochs):\n",
    "        model.train()\n",
    "        i = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = Variable(inputs).to(device=device), Variable(labels).to(device=device)\n",
    "\n",
    "            tr_loss.append(model_step(model, optimizer, criterion, inputs, labels))\n",
    "            if optimizer.__class__.__name__ == 'SUG':\n",
    "                lips.append(optimizer.get_lipsitz_const())\n",
    "                grad.append(optimizer.get_sq_grad)\n",
    "        times.append(time_since(start_time))\n",
    "        if ep % print_every == 0:\n",
    "            print(\"Epoch {}, training loss {}, time passed {}\".format(ep, sum(tr_loss[-i:]) / i, time_since(start_time)))\n",
    "\n",
    "        if validloader is None:\n",
    "            continue\n",
    "        model.zero_grad()\n",
    "        model.eval()\n",
    "        j = 0\n",
    "        for j, data in enumerate(validloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device=device), labels.to(device=device)\n",
    "            val_loss.append(model_step(model, optimizer, criterion, inputs, labels))\n",
    "        if ep % print_every == 0:\n",
    "            print(\"Validation loss {}\".format(sum(val_loss[-j:]) / j))\n",
    "        \n",
    "    return tr_loss, times, val_loss, lips, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sz4svVB3vNyZ"
   },
   "outputs": [],
   "source": [
    "print_every = 2\n",
    "n_epochs = 6\n",
    "tr_loss = {}\n",
    "tr_loss['accSGD'] = {}\n",
    "tr_loss['Adam'] = {}\n",
    "tr_loss['amsgrad'] = {}\n",
    "tr_loss['sug'] = {}\n",
    "tr_loss['A2GradInc'] = {}\n",
    "tr_loss['A2GradUni'] = {}\n",
    "tr_loss['A2GradExp'] = {}\n",
    "val_loss = {}\n",
    "val_loss['accSGD'] = {}\n",
    "val_loss['Adam'] = {}\n",
    "val_loss['amsgrad'] = {}\n",
    "val_loss['sug'] = {}\n",
    "val_loss['A2GradInc'] = {}\n",
    "val_loss['A2GradUni'] = {}\n",
    "val_loss['A2GradExp'] = {}\n",
    "lrs = [0.001]#[0.1, 0.01, 0.001]\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6jCFM66X7ql"
   },
   "outputs": [],
   "source": [
    "def concat_states(state1, state2):\n",
    "    states = {\n",
    "            'epoch': state1['epoch'] + state2['epoch'],\n",
    "            'state_dict': state2['state_dict'],\n",
    "            'optimizer': state2['optimizer'],\n",
    "            'tr_loss' : state1['tr_loss'] + state2['tr_loss'],\n",
    "            'val_loss' : state1['val_loss'] + state2['val_loss'],\n",
    "            'lips' : state1['lips'] + state2['lips'],\n",
    "            'grad' : state1['grad'] + state2['grad'],\n",
    "            #'times' : state1['times'] + list(map(lambda x: x + state1['times'][-1],state2['times']))\n",
    "             'times' : state1['times'] + state2['times']\n",
    "             }\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98EHtawSzoTM"
   },
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "hTMWs2dYznyO",
    "outputId": "01e33386-4143-4fe1-ead2-ef3ccd13cea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam  lr=0.001:\n",
      "Epoch 0, training loss 0.4471673777217208, time passed 0m 17s\n",
      "Validation loss 0.48481341447177106\n",
      "Epoch 2, training loss 0.39360389989321554, time passed 0m 54s\n",
      "Validation loss 0.4692441363024516\n",
      "Epoch 4, training loss 0.38341145817109673, time passed 1m 35s\n",
      "Validation loss 0.5846125006058818\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model = LR()\n",
    "    print(\"Adam  lr={}:\".format(lr))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    tr_loss['Adam'][lr], times, val_loss['Adam'][lr], lips, grad = train(model, trainloader, criterion, \n",
    "                                                                         optimizer, n_epochs=n_epochs, \n",
    "                                                                         print_every=print_every, \n",
    "                                                                         validloader=validloader)\n",
    "    states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['Adam'][lr],\n",
    "            'val_loss' : val_loss['Adam'][lr],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "             }\n",
    "    torch.save(states, './MNIST/LR_Adam_' + str(lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2GradInc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2GradInc\n",
      "Epoch 0, training loss 0.6602763776485593, time passed 0m 21s\n",
      "Validation loss 0.5054242114502093\n",
      "Epoch 2, training loss 0.4349289565605807, time passed 1m 2s\n",
      "Validation loss 0.4237436142879699\n",
      "Epoch 4, training loss 0.400614009719051, time passed 1m 48s\n",
      "Validation loss 0.3957431950432896\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model = LR()\n",
    "    print('A2GradInc')\n",
    "    optimizer = A2GradInc(model.parameters())\n",
    "    tr_loss['A2GradInc'][lr], times, val_loss['A2GradInc'][lr], lips, grad = train(model, trainloader, criterion, \n",
    "                                                                         optimizer, n_epochs=n_epochs, \n",
    "                                                                         print_every=print_every, \n",
    "                                                                         validloader=validloader)\n",
    "    states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['A2GradInc'][lr],\n",
    "            'val_loss' : val_loss['A2GradInc'][lr],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "             }\n",
    "    torch.save(states, './MNIST/LR_A2GradInc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2GradExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2GradExp  lr=0.001:\n",
      "Epoch 0, training loss 0.890304833246765, time passed 0m 16s\n",
      "Validation loss 0.6959082544174604\n",
      "Epoch 2, training loss 0.5943733410107238, time passed 1m 4s\n",
      "Validation loss 0.5783207188677183\n",
      "Epoch 4, training loss 0.5419563035308936, time passed 1m 45s\n",
      "Validation loss 0.5347908401390535\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model = LR()\n",
    "    print(\"A2GradExp  lr={}:\".format(lr))\n",
    "    optimizer = A2GradExp(model.parameters(), beta=10, lips=10, rho=0.9)\n",
    "    tr_loss['A2GradExp'][lr], times, val_loss['A2GradExp'][lr], lips, grad = train(model, trainloader, criterion, \n",
    "                                                                         optimizer, n_epochs=n_epochs, \n",
    "                                                                         print_every=print_every, \n",
    "                                                                         validloader=validloader)\n",
    "    states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['A2GradExp'][lr],\n",
    "            'val_loss' : val_loss['A2GradExp'][lr],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "             }\n",
    "    torch.save(states, './MNIST/LR_A2GradExp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2GradUni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2GradUni  lr=0.001:\n",
      "Epoch 0, training loss 0.8777040629732776, time passed 0m 21s\n",
      "Validation loss 0.6466419323036516\n",
      "Epoch 2, training loss 0.5362675025063032, time passed 1m 2s\n",
      "Validation loss 0.5200954246636614\n",
      "Epoch 4, training loss 0.4826502767078481, time passed 1m 52s\n",
      "Validation loss 0.47515102096064293\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model = LR()\n",
    "    print(\"A2GradUni  lr={}:\".format(lr))\n",
    "    optimizer = A2GradUni(model.parameters())\n",
    "    tr_loss['A2GradUni'][lr], times, val_loss['A2GradUni'][lr], lips, grad = train(model, trainloader, criterion, \n",
    "                                                                         optimizer, n_epochs=n_epochs, \n",
    "                                                                         print_every=print_every, \n",
    "                                                                         validloader=validloader)\n",
    "    states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['A2GradUni'][lr],\n",
    "            'val_loss' : val_loss['A2GradUni'][lr],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "             }\n",
    "    torch.save(states, './MNIST/LR_A2GradUni')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AMSgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b1CbtL-F0xaF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss 0.4065497466627148, time passed 0m 19s\n",
      "Validation loss 0.42612661495290843\n",
      "Epoch 2, training loss 0.32533949986387606, time passed 1m 10s\n",
      "Validation loss 0.37711557346290964\n",
      "Epoch 4, training loss 0.31078923913632767, time passed 1m 55s\n",
      "Validation loss 0.3057402240929971\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model = LR()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.99), eps=1e-8, amsgrad=True)\n",
    "    tr_loss['amsgrad'], times, val_loss['amsgrad'], lips, grad = train(model, trainloader, criterion, optimizer, \n",
    "                                                           n_epochs=n_epochs, print_every=print_every, \n",
    "                                                           validloader=validloader)\n",
    "                                                         \n",
    "    states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['amsgrad'],\n",
    "            'val_loss' : val_loss['amsgrad'],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "         }\n",
    "    torch.save(states, './MNIST/LR_amsgrad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accelerated SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss 0.40764180013083695, time passed 0m 20s\n",
      "Validation loss 0.40028829447166864\n",
      "Epoch 2, training loss 0.30266382122565666, time passed 1m 7s\n",
      "Validation loss 0.36440590935486555\n",
      "Epoch 4, training loss 0.28831493548777254, time passed 1m 53s\n",
      "Validation loss 0.31914368522866027\n"
     ]
    }
   ],
   "source": [
    "model = LR()\n",
    "optimizer = AccSGD(model.parameters(), lr=0.001, kappa = 1000.0, xi = 10.0)\n",
    "tr_loss['accSGD'], times, val_loss['accSGD'], lips, grad = train(model, trainloader, criterion, optimizer, \n",
    "                                                           n_epochs=n_epochs, print_every=print_every, \n",
    "                                                           validloader=validloader)\n",
    "                                          \n",
    "states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['accSGD'],\n",
    "            'val_loss' : val_loss['accSGD'],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "         }\n",
    "torch.save(states, './MNIST/LR_accSGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss 0.46343427602327797, time passed 0m 34s\n",
      "Validation loss 0.6206142763416612\n",
      "Epoch 2, training loss 0.3912737628173737, time passed 1m 51s\n",
      "Validation loss 0.3956923408834107\n",
      "Epoch 4, training loss 0.3867288236060278, time passed 3m 7s\n",
      "Validation loss 0.5360902712203098\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model = LR()\n",
    "    optimizer = SUG(model.parameters(), l_0=lr, momentum=0, nesterov=False)\n",
    "    tr_loss['sug'], times, val_loss['sug'], lips, grad = train(model, trainloader, criterion, optimizer, \n",
    "                                                           n_epochs=n_epochs, print_every=print_every, \n",
    "                                                           validloader=validloader)\n",
    "    states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['sug'],\n",
    "            'val_loss' : val_loss['sug'],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "         }\n",
    "    torch.save(states, './MNIST/LR_sug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJ24b8FFpIQU"
   },
   "source": [
    "### FC neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zBxqCh7YEOok"
   },
   "outputs": [],
   "source": [
    "n_epochs = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "KlkSl5x-vPRW",
    "outputId": "0c739f08-c0ac-4326-b69d-870d51daa4a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam  lr=0.001:\n",
      "Epoch 0, training loss 0.31472805445884217, time passed 2m 17s\n",
      "Validation loss 0.33562005795256833\n",
      "Epoch 2, training loss 0.1640293464746599, time passed 7m 56s\n",
      "Validation loss 0.16987620446443386\n",
      "Epoch 4, training loss 0.1375037254112192, time passed 13m 44s\n",
      "Validation loss 0.1692539849275083\n",
      "Epoch 6, training loss 0.12588283565029146, time passed 20m 30s\n",
      "Validation loss 0.17024462949287886\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model = FC()\n",
    "    print(\"Adam  lr={}:\".format(lr))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    tr_loss['Adam'][lr], times, val_loss['Adam'][lr], lips, grad = train(model, trainloader, criterion, \n",
    "                                                                         optimizer, n_epochs=n_epochs, \n",
    "                                                                         print_every=print_every, \n",
    "                                                                         validloader=validloader)\n",
    "    states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['Adam'][lr],\n",
    "            'val_loss' : val_loss['Adam'][lr],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "             }\n",
    "    torch.save(states, './MNIST/FC_Adam_' + str(lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AMSgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ODga9MkTvnRl",
    "outputId": "e1949814-6533-4078-9b5e-cf193b98398f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss 0.2954024291499492, time passed 2m 8s\n",
      "Validation loss 0.19679289057075633\n",
      "Epoch 2, training loss 0.11406174258036991, time passed 7m 39s\n",
      "Validation loss 0.14818219451598985\n",
      "Epoch 4, training loss 0.07966713166552923, time passed 13m 13s\n",
      "Validation loss 0.10309265803739515\n",
      "Epoch 6, training loss 0.06052994809905093, time passed 18m 52s\n",
      "Validation loss 0.10621145213906528\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model = FC()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.99), eps=1e-8, amsgrad=True)\n",
    "    tr_loss['amsgrad'], times, val_loss['amsgrad'], lips, grad = train(model, trainloader, criterion, optimizer, \n",
    "                                                           n_epochs=n_epochs, print_every=print_every, \n",
    "                                                           validloader=validloader)\n",
    "                                                         \n",
    "    states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['amsgrad'],\n",
    "            'val_loss' : val_loss['amsgrad'],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "         }\n",
    "    torch.save(states, './MNIST/FC_amsgrad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accelerated SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8y30s5VE4px"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss 0.4004168582505848, time passed 0m 36s\n",
      "Validation loss 0.26797187960428787\n",
      "Epoch 2, training loss 0.15343301741924836, time passed 1m 55s\n",
      "Validation loss 0.17337015810517428\n",
      "Epoch 4, training loss 0.10210626035243803, time passed 3m 13s\n",
      "Validation loss 0.1559533597549474\n",
      "Epoch 6, training loss 0.07533009543270398, time passed 4m 37s\n",
      "Validation loss 0.10859271298254157\n"
     ]
    }
   ],
   "source": [
    "model = FC()\n",
    "optimizer = AccSGD(model.parameters(), lr=0.001, kappa = 1000.0, xi = 10.0)\n",
    "tr_loss['accSGD'], times, val_loss['accSGD'], lips, grad = train(model, trainloader, criterion, optimizer, \n",
    "                                                           n_epochs=n_epochs, print_every=print_every, \n",
    "                                                           validloader=validloader)\n",
    "                                          \n",
    "states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['accSGD'],\n",
    "            'val_loss' : val_loss['accSGD'],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "         }\n",
    "torch.save(states, './MNIST/FC_accSGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss 0.31559966881047585, time passed 1m 10s\n",
      "Validation loss 0.21523238326931807\n",
      "Epoch 2, training loss 0.12110273715224637, time passed 3m 56s\n",
      "Validation loss 0.12730560067125085\n",
      "Epoch 4, training loss 0.08468967698603876, time passed 7m 5s\n",
      "Validation loss 0.0998345566304751\n",
      "Epoch 6, training loss 0.0672137110604895, time passed 9m 58s\n",
      "Validation loss 0.08988766150243445\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model = FC()\n",
    "    optimizer = SUG(model.parameters(), l_0=lr, momentum=0, nesterov=False)\n",
    "    tr_loss['sug'], times, val_loss['sug'], lips, grad = train(model, trainloader, criterion, optimizer, \n",
    "                                                           n_epochs=n_epochs, print_every=print_every, \n",
    "                                                           validloader=validloader)\n",
    "    states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['sug'],\n",
    "            'val_loss' : val_loss['sug'],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "         }\n",
    "    torch.save(states, './MNIST/FC_sug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2GradInc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2GradInc\n",
      "Epoch 0, training loss 0.3405313328328119, time passed 0m 50s\n",
      "Validation loss 0.40539793937192486\n",
      "Epoch 2, training loss 0.2502931321926777, time passed 2m 36s\n",
      "Validation loss 0.25179623060090744\n",
      "Epoch 4, training loss 0.19747851773686237, time passed 4m 52s\n",
      "Validation loss 0.2213085105865465\n",
      "Epoch 6, training loss 0.19407693164069398, time passed 6m 41s\n",
      "Validation loss 0.20182552583856866\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model = FC()\n",
    "    print('A2GradInc')\n",
    "    optimizer = A2GradInc(model.parameters(), beta=1, lips=10)\n",
    "    tr_loss['A2GradInc'][lr], times, val_loss['A2GradInc'][lr], lips, grad = train(model, trainloader, criterion, \n",
    "                                                                         optimizer, n_epochs=n_epochs, \n",
    "                                                                         print_every=print_every, \n",
    "                                                                         validloader=validloader)\n",
    "    states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['A2GradInc'][lr],\n",
    "            'val_loss' : val_loss['A2GradInc'][lr],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "             }\n",
    "    torch.save(states, './MNIST/FC_A2GradInc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2GradUni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2GradUni  lr=0.001:\n",
      "Epoch 0, training loss 0.3645090667142411, time passed 0m 49s\n",
      "Validation loss 0.3461166744949872\n",
      "Epoch 2, training loss 0.2715470625102375, time passed 2m 48s\n",
      "Validation loss 0.25930503801856375\n",
      "Epoch 4, training loss 0.21909287481441134, time passed 4m 37s\n",
      "Validation loss 0.20832079588015273\n",
      "Epoch 6, training loss 0.1821865990205095, time passed 6m 28s\n",
      "Validation loss 0.17309671778210325\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model = FC()\n",
    "    print(\"A2GradUni  lr={}:\".format(lr))\n",
    "    optimizer = A2GradUni(model.parameters(), beta=1, lips=10)\n",
    "    tr_loss['A2GradUni'][lr], times, val_loss['A2GradUni'][lr], lips, grad = train(model, trainloader, criterion, \n",
    "                                                                         optimizer, n_epochs=n_epochs, \n",
    "                                                                         print_every=print_every, \n",
    "                                                                         validloader=validloader)\n",
    "    states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['A2GradUni'][lr],\n",
    "            'val_loss' : val_loss['A2GradUni'][lr],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "             }\n",
    "    torch.save(states, './MNIST/FC_A2GradUni')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2GradExp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2GradExp  lr=0.001:\n",
      "Epoch 0, training loss 0.38943213840140223, time passed 0m 48s\n",
      "Validation loss 0.3454775995363284\n",
      "Epoch 2, training loss 0.289244648559167, time passed 2m 49s\n",
      "Validation loss 0.28307165995922656\n",
      "Epoch 4, training loss 0.2758141870881653, time passed 4m 34s\n",
      "Validation loss 0.2541869924883675\n",
      "Epoch 6, training loss 0.2549206986760371, time passed 6m 14s\n",
      "Validation loss 0.2627989425029475\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model = FC()\n",
    "    print(\"A2GradExp  lr={}:\".format(lr))\n",
    "    optimizer = A2GradExp(model.parameters(), beta=1, lips=5, rho=0.99)\n",
    "    tr_loss['A2GradExp'][lr], times, val_loss['A2GradExp'][lr], lips, grad = train(model, trainloader, criterion, \n",
    "                                                                         optimizer, n_epochs=n_epochs, \n",
    "                                                                         print_every=print_every, \n",
    "                                                                         validloader=validloader)\n",
    "    states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['A2GradExp'][lr],\n",
    "            'val_loss' : val_loss['A2GradExp'][lr],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "             }\n",
    "    torch.save(states, './MNIST/FC_A2GradExp')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MNIST.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
