{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fiSHwkvZumRd"
   },
   "outputs": [],
   "source": [
    "from util import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p2Mo1UdyumT9",
    "outputId": "f14053f0-1237-42a4-f9ee-f5c16460b496"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./MNIST\"\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACC ADD GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "import copy\n",
    "\n",
    "class AccSGD(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=required, kappa = 1000.0, xi = 10.0, smallConst = 0.7, weight_decay=0):\n",
    "        defaults = dict(lr=lr, kappa=kappa, xi=xi, smallConst=smallConst,\n",
    "                        weight_decay=weight_decay)\n",
    "        super(AccSGD, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AccSGD, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\" Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            large_lr = (group['lr']*group['kappa'])/(group['smallConst'])\n",
    "            Alpha = 1.0 - ((group['smallConst']*group['smallConst']*group['xi'])/group['kappa'])\n",
    "            Beta = 1.0 - Alpha\n",
    "            zeta = group['smallConst']/(group['smallConst']+Beta)\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "                param_state = self.state[p]\n",
    "                if 'momentum_buffer' not in param_state:\n",
    "                    param_state['momentum_buffer'] = copy.deepcopy(p.data)\n",
    "                buf = param_state['momentum_buffer']\n",
    "                buf.mul_((1.0/Beta)-1.0)\n",
    "                buf.add_(-large_lr,d_p)\n",
    "                buf.add_(p.data)\n",
    "                buf.mul_(Beta)\n",
    "\n",
    "                p.data.add_(-group['lr'],d_p)\n",
    "                p.data.mul_(zeta)\n",
    "                p.data.add_(1.0-zeta,buf)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6Lk5gvGu1Yc"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_rk3Q_Jku2by"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='../data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "valid_dataset = torchvision.datasets.MNIST(root='../data', train=True, \n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='../data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bYLS8LcHWv7n"
   },
   "outputs": [],
   "source": [
    "valid_size=0.15\n",
    "num_train = len(trainset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, \n",
    "               batch_size=batch_size, sampler=train_sampler,\n",
    "               num_workers=2)\n",
    "\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, \n",
    "               batch_size=batch_size, sampler=valid_sampler,\n",
    "               num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "7j9cjtv5u57l",
    "outputId": "2d6a7e4b-aa09-49fd-9dbd-d0e3f65ff814"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  tensor([0, 0, 2, 4])\n",
      "Batch shape:  torch.Size([4, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_batch(batch):\n",
    "    im = torchvision.utils.make_grid(batch)\n",
    "    plt.imshow(np.transpose(im.numpy(), (1, 2, 0)))\n",
    "    \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print('Labels: ', labels)\n",
    "print('Batch shape: ', images.size())\n",
    "show_batch(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THk9uDZku-gM"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnS_0OICbHF4"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6micuYGbGOL"
   },
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LR, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = F.log_softmax(self.linear1(x.view(batch_size, -1)), -1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A4rjRazJa-S6"
   },
   "source": [
    "###   Two-layer neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uh3HnmUmu96Q"
   },
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FC, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 256)\n",
    "        self.linear2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_relu = F.relu(self.linear1(x.view(batch_size, -1)))\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SSTdIH_vKV5"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nMttjLlZrbcd"
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def model_step(model, optimizer, criterion, inputs, labels):\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    if model.training:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "    if optimizer.__class__.__name__ != 'SUG':\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            upd_outputs = model(inputs)\n",
    "            upd_loss = criterion(upd_outputs, labels)\n",
    "            upd_loss.backward()\n",
    "            return upd_loss\n",
    "\n",
    "        optimizer.step(loss, closure)\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMgATVQnvIiy"
   },
   "outputs": [],
   "source": [
    "def train(model, trainloader, criterion, optimizer, n_epochs=2, validloader=None, eps=1e-5, print_every=1):\n",
    "    tr_loss, val_loss, lips, times, grad, acc = ([] for i in range(6))\n",
    "    start_time = time.time()\n",
    "    model.to(device=device)\n",
    "    for ep in range(n_epochs):\n",
    "        model.train()\n",
    "        i = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = Variable(inputs).to(device=device), Variable(labels).to(device=device)\n",
    "\n",
    "            tr_loss.append(model_step(model, optimizer, criterion, inputs, labels))\n",
    "            if optimizer.__class__.__name__ == 'SUG':\n",
    "                lips.append(optimizer.get_lipsitz_const())\n",
    "                grad.append(optimizer.get_sq_grad)\n",
    "        times.append(time_since(start_time))\n",
    "        if ep % print_every == 0:\n",
    "            print(\"Epoch {}, training loss {}, time passed {}\".format(ep, sum(tr_loss[-i:]) / i, time_since(start_time)))\n",
    "\n",
    "        if validloader is None:\n",
    "            continue\n",
    "        model.zero_grad()\n",
    "        model.eval()\n",
    "        j = 0\n",
    "        for j, data in enumerate(validloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device=device), labels.to(device=device)\n",
    "            val_loss.append(model_step(model, optimizer, criterion, inputs, labels))\n",
    "        if ep % print_every == 0:\n",
    "            print(\"Validation loss {}\".format(sum(val_loss[-j:]) / j))\n",
    "        \n",
    "    return tr_loss, times, val_loss, lips, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sz4svVB3vNyZ"
   },
   "outputs": [],
   "source": [
    "print_every = 2\n",
    "n_epochs = 6\n",
    "tr_loss = {}\n",
    "tr_loss['accSGD'] = {}\n",
    "val_loss = {}\n",
    "val_loss['accSGD'] = {}\n",
    "lrs = [0.001]#[0.1, 0.01, 0.001]\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6jCFM66X7ql"
   },
   "outputs": [],
   "source": [
    "def concat_states(state1, state2):\n",
    "    states = {\n",
    "            'epoch': state1['epoch'] + state2['epoch'],\n",
    "            'state_dict': state2['state_dict'],\n",
    "            'optimizer': state2['optimizer'],\n",
    "            'tr_loss' : state1['tr_loss'] + state2['tr_loss'],\n",
    "            'val_loss' : state1['val_loss'] + state2['val_loss'],\n",
    "            'lips' : state1['lips'] + state2['lips'],\n",
    "            'grad' : state1['grad'] + state2['grad'],\n",
    "            #'times' : state1['times'] + list(map(lambda x: x + state1['times'][-1],state2['times']))\n",
    "             'times' : state1['times'] + state2['times']\n",
    "             }\n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98EHtawSzoTM"
   },
   "source": [
    "### LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "hTMWs2dYznyO",
    "outputId": "01e33386-4143-4fe1-ead2-ef3ccd13cea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam  lr=0.001:\n",
      "Epoch 0, training loss 0.44380961404364744, time passed 0m 16s\n",
      "Validation loss 0.392925121534705\n",
      "Epoch 2, training loss 0.38799097983733954, time passed 0m 58s\n",
      "Validation loss 0.46064660396196094\n",
      "Epoch 4, training loss 0.38396756426377787, time passed 1m 35s\n",
      "Validation loss 0.40814950427506946\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Adam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-415778316ea1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                                          \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                                          \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprint_every\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                                                          validloader=validloader)\n\u001b[0m\u001b[1;32m      9\u001b[0m     states = {\n\u001b[1;32m     10\u001b[0m             \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Adam'"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model = LR()\n",
    "    print(\"Adam  lr={}:\".format(lr))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    tr_loss['Adam'][lr], times, val_loss['Adam'][lr], lips, grad = train(model, trainloader, criterion, \n",
    "                                                                         optimizer, n_epochs=n_epochs, \n",
    "                                                                         print_every=print_every, \n",
    "                                                                         validloader=validloader)\n",
    "    states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['Adam'][lr],\n",
    "            'val_loss' : val_loss['Adam'][lr],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "             }\n",
    "    torch.save(states, './MNIST/LR_Adam_' + str(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b1CbtL-F0xaF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss 1.5391756051041332, time passed 0m 18s\n",
      "Validation loss 3.350540576000443\n",
      "Epoch 4, training loss 1.3543622778394757, time passed 1m 49s\n",
      "Validation loss 2.195955710212499\n",
      "Epoch 8, training loss 1.2929582049769992, time passed 3m 13s\n",
      "Validation loss 3.299478966680603\n"
     ]
    }
   ],
   "source": [
    "#from .optimizer import Optimizer, required\n",
    "l_0 = 2\n",
    "for lr in lrs:\n",
    "    model = LR()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.99), eps=1e-8, amsgrad=True)\n",
    "#optimizer = AccAdSGD(model.parameters(), lr=0.01, kappa = 1000.0, xi = 10.0)\n",
    "    tr_loss['amsgrad'], times, val_loss['amsgrad'], lips, grad = train(model, trainloader, criterion, optimizer, \n",
    "                                                           n_epochs=n_epochs, print_every=print_every, \n",
    "                                                           validloader=validloader)\n",
    "#optimizer.zero_grad()\n",
    "#loss_fn(model(input), target).backward()\n",
    "#optimizer.step()\n",
    "\n",
    "#optimizer = SUG(model.parameters(), l_0=l_0, momentum=0.)\n",
    "#tr_loss['sug'], times, val_loss['sug'], lips, grad = train(model, trainloader, criterion, optimizer, \n",
    "#                                                           n_epochs=n_epochs, print_every=print_every, \n",
    "#                                                           validloader=validloader)\n",
    "states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['amsgrad'],\n",
    "            'val_loss' : val_loss['amsgrad'],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "         }\n",
    "torch.save(states, './MNIST/LR_amsgrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss 0.5080414829483656, time passed 0m 18s\n",
      "Validation loss 1.2798434100222031\n",
      "Epoch 4, training loss 0.4614034309630066, time passed 1m 44s\n",
      "Validation loss 1.7207055731846763\n",
      "Epoch 8, training loss 0.4545040327209192, time passed 2m 57s\n",
      "Validation loss 1.2367687740711177\n"
     ]
    }
   ],
   "source": [
    "l_0 = 2\n",
    "model = LR()\n",
    "optimizer = AccSGD(model.parameters(), lr=0.01, kappa = 1000.0, xi = 10.0)\n",
    "tr_loss['AccSGD'], times, val_loss['AccSGD'], lips, grad = train(model, trainloader, criterion, optimizer, \n",
    "                                                           n_epochs=n_epochs, print_every=print_every, \n",
    "                                                           validloader=validloader)\n",
    "                                          \n",
    "states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['AccSGD'],\n",
    "            'val_loss' : val_loss['AccSGD'],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "         }\n",
    "torch.save(states, './MNIST/LR_AccSGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJ24b8FFpIQU"
   },
   "source": [
    "### FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zBxqCh7YEOok"
   },
   "outputs": [],
   "source": [
    "n_epochs = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "KlkSl5x-vPRW",
    "outputId": "0c739f08-c0ac-4326-b69d-870d51daa4a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD  lr=0.05, momentum=0. :\n",
      "Epoch 0, training loss 0.2366863005871361, time passed 0m 29s\n",
      "Validation loss 0.11924165910008008\n",
      "Epoch 4, training loss 0.036543241790046185, time passed 2m 44s\n",
      "Validation loss 0.02659412257774187\n",
      "Epoch 8, training loss 0.009252302931265622, time passed 4m 57s\n",
      "Validation loss 0.007337128495894416\n",
      "SGD  lr=0.01, momentum=0. :\n",
      "Epoch 0, training loss 0.390318907132868, time passed 0m 28s\n",
      "Validation loss 0.21850812987732113\n",
      "Epoch 4, training loss 0.07997714770486358, time passed 2m 43s\n",
      "Validation loss 0.06550830985610355\n",
      "Epoch 8, training loss 0.04263166229681311, time passed 4m 59s\n",
      "Validation loss 0.0345691220396728\n",
      "SGD  lr=0.005, momentum=0. :\n",
      "Epoch 0, training loss 0.49476544181125587, time passed 0m 28s\n",
      "Validation loss 0.2863557395032906\n",
      "Epoch 4, training loss 0.13017870467198542, time passed 2m 44s\n",
      "Validation loss 0.11114349794313609\n",
      "Epoch 8, training loss 0.0772063676231337, time passed 4m 57s\n",
      "Validation loss 0.06647039221731384\n"
     ]
    }
   ],
   "source": [
    "for lr in lrs:\n",
    "    model = FC()\n",
    "    print(\"SGD  lr={}, momentum=0. :\".format(lr))\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.)\n",
    "    tr_loss['sgd'][lr], times, val_loss['sgd'][lr], lips, grad = train(model, trainloader, criterion, optimizer, n_epochs=n_epochs, print_every=print_every, validloader=validloader)\n",
    "    states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['sgd'][lr],\n",
    "            'val_loss' : val_loss['sgd'][lr],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "             }\n",
    "    torch.save(states, './MNIST/FC_' + str(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ODga9MkTvnRl",
    "outputId": "e1949814-6533-4078-9b5e-cf193b98398f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss 0.2193387469148363, time passed 0m 59s\n",
      "Validation loss 0.11775196808452021\n",
      "Epoch 4, training loss 0.03766450093525402, time passed 5m 24s\n",
      "Validation loss 0.030662210186940927\n",
      "Epoch 8, training loss 0.017794447657136097, time passed 9m 37s\n",
      "Validation loss 0.013901273798232293\n"
     ]
    }
   ],
   "source": [
    "l_0 = 2\n",
    "model = FC()\n",
    "optimizer = SUG(model.parameters(), l_0=l_0, momentum=0.)\n",
    "tr_loss['sug'], times, val_loss['sug'], lips, grad = train(model, trainloader, criterion, optimizer, n_epochs=n_epochs, print_every=print_every, validloader=validloader)\n",
    "states = {\n",
    "            'epoch': n_epochs,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'tr_loss' : tr_loss['sug'],\n",
    "            'val_loss' : val_loss['sug'],\n",
    "            'lips' : lips,\n",
    "            'grad' : grad,\n",
    "            'times' : times\n",
    "         }\n",
    "torch.save(states, './MNIST/FC_sug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8y30s5VE4px"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MNIST.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
